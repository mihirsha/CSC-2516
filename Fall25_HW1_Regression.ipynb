{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mihirsha/CSC-2516/blob/homework1/Fall25_HW1_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 1 - Regression\n",
        "CSC413/2516: Neural Networks and Deep Learning\n",
        "\n",
        "Throughout the semester, you will be assigned weekly homeworks that you can submit asynchronously. Any questions regarding assignments can be asked on Piazza (with tag hw1) or during office hours.\n",
        "\n",
        "Code cells marked with ## Your Code must be completed by you. You should only change the parts between `##########################################`.\n",
        "Your implementation is autograded against test cases. You can run auto tests any time, be aware that you have 10 tokens per 12 hours (they regenerate every 12 hours) per homework.\n",
        "\n",
        "Please disclose any external help or collaboration in the cell provided at the end of the notebook.\n",
        "\n",
        "If we update the notebook we will update the version below.\n",
        "\n",
        "**Notebook Version:** 1.0\n",
        "\n",
        "CHANGELOG:\n",
        "- Initial release\n",
        "\n",
        "\n",
        "### To Edit\n",
        "\n",
        "Navigate to the top of the page and select `File > Download` or `File > Save a copy in Drive` to edit.\n",
        "\n",
        "Replace \"## Your Code\" lines with your implementation.\n",
        "\n",
        "### To submit\n",
        "\n",
        "You should download the notebook using `File > Download > Download .py`. You will be able to run tests and get immediate feedback upon submission.\n",
        "\n",
        "### Overview\n",
        "In this homework you will implement linear regression (Section 1) and logistic (Section 2) regression from scratch."
      ],
      "metadata": {
        "id": "YzVlV7GwxyvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set-up\n",
        "! Do not change the following code"
      ],
      "metadata": {
        "id": "Dn1NNDC6eQiB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouAwSuZYxk0r"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Tuple, Callable, Literal\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_data():\n",
        "    size = 1000\n",
        "    x = np.linspace(0, 1, size)\n",
        "    eps = np.random.normal(loc=0, scale=0.1, size=size)\n",
        "    y = -10*x + 5 + eps\n",
        "    y += 15*np.logical_and(x > 0.75, x < 0.8).astype(float)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "daS3zCuRy68H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Implement Linear Regression (4.5 Points)\n",
        "Implement a basic linear regression model to fit the data from `generate_data` using gradient descent.\n"
      ],
      "metadata": {
        "id": "z26BJQskzPJM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Visualize Data (0.5 Points)\n",
        "First start by creating a scatter plot of the data."
      ],
      "metadata": {
        "id": "NpkAV31W4D02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_data(x, y):\n",
        "    fig, ax = plt.subplots(figsize=(4, 2))\n",
        "    ##########################################\n",
        "    ## TODO: Create a scatter plot of the data\n",
        "    ## Your Code, 0.5 points\n",
        "\n",
        "    ##########################################\n",
        "    return fig, ax"
      ],
      "metadata": {
        "id": "8PeZeLaH4IBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(SEED)\n",
        "inputs, targets = generate_data()\n",
        "fig, ax = visualize_data(inputs, targets)"
      ],
      "metadata": {
        "id": "QjZb6bat4cYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Implement Linear Regression (3 Points)\n",
        "Implement a simple linear regression model using gradient descent to fit the data from the `generate_data` function. Use `numpy` only and derive all gradients manually --don't use autograd tools (PyTorch, TensorFlow, MXNet, JAX, etc.). The autograding environment will not have any autograd libraries installed.\n",
        "\n",
        "**Model:** Your model should take the standard linear form:\n",
        "$$\n",
        "\\hat y = f(w,b,x):= w \\cdot x + b, \\tag{1}\n",
        "$$\n",
        "where $y \\in \\mathbb R$ is the predicted output for the input $x \\in \\mathbb R$, $w \\in \\mathbb R$ is a scalar weight parameter, and $b \\in \\mathbb R$ is a bias parameter.\n",
        "\n",
        "**Objective:** Minimize the mean squared error (MSE) between predictions and targets:\n",
        "$$\\mathscr{L}(w,b) = \\frac 1 {2n} \\sum_{i=1}^n \\left(y_i-\\hat y_i\\right)^2\\tag{MSE}$$\n",
        "\n",
        "**Updates:** Remember our goal is to find parameters $(w^\\ast, b^\\ast)$ that minimize the objective function. In gradient descent we iteratively update the parameters in the direction of the negative gradient, where\n",
        "$$w_t \\gets w_{t-1} - \\eta \\nabla_{w_{t-1}} \\mathscr L \\tag{3}$$\n",
        "$$b_t \\gets b_{t-1} - \\eta \\nabla_{b_{t-1}} \\mathscr L \\tag{4}$$\n",
        "\n",
        "\n",
        "**Derivatives:** You must compute:\n",
        "- The intermediate gradient $\\frac {\\partial \\mathscr{L}}{\\partial \\hat{y}} \\tag{5}$\n",
        "- Gradient of the loss with respect to the weight $w$: $\\frac {\\partial \\mathscr{L}}{\\partial w} \\tag{6}$\n",
        "- Gradient of the loss with respect to the bias $b$: $\\frac {\\partial \\mathscr{L}}{\\partial b} \\tag{7}$\n",
        "\n",
        "You are welcome to use any technique you want to decide when to stop training. Make sure you tune your optimization hyper-parameters so that the model converges. Print out or plot the loss over the course of training.\n"
      ],
      "metadata": {
        "id": "CwpbkSZZZLJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Don't change initial parameters\n",
        "weight = 1.\n",
        "bias = 0."
      ],
      "metadata": {
        "id": "JB24-cB7dztL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model(weight: float, bias: float, inputs: np.array) -> float:\n",
        "    ##########################################\n",
        "    ## TODO: Implement linear model: y = weight * x + bias (Eq. 1)\n",
        "    ## 0.5 pts\n",
        "    ## Your Code\n",
        "    res = ...\n",
        "    ##########################################\n",
        "    return res\n",
        "\n",
        "def loss(weight: float, bias: float, inputs: np.array, targets: np.array) -> float:\n",
        "    ##########################################\n",
        "    ## TODO: Implement mean squared error loss (Eq. MSE)\n",
        "    ## 0.5 pts\n",
        "    ## Your Code\n",
        "    res = ...\n",
        "    ##########################################\n",
        "    return res\n",
        "\n",
        "def dloss_dmodel(weight: float, bias: float, inputs: np.array, targets: np.array) -> np.array:\n",
        "    ##########################################\n",
        "    ## TODO: Compute the derivative of the loss function\n",
        "    ## with respect to the model output (Eq. 5)\n",
        "    ## 0.5 pts\n",
        "    ## Your Code\n",
        "    res = ...\n",
        "    ##########################################\n",
        "    return res\n",
        "\n",
        "def weight_update(weight: float, bias: float, inputs: np.array, targets: np.array) -> float:\n",
        "    ##########################################\n",
        "    ## TODO: Compute the update to be applied to the weights (Eq. 3)\n",
        "    ## We will use this function to update the weights in `train_linear_regression` function (see below):\n",
        "    # weight - learning_rate*weight_update(weight, bias, inputs, targets)\n",
        "    ## 0.5 pts\n",
        "    ## Your Code\n",
        "    res = ...\n",
        "    ##########################################\n",
        "    return res\n",
        "\n",
        "def bias_update(weight: float, bias: float, inputs: np.array, targets: np.array) -> float:\n",
        "    ##########################################\n",
        "    ## TODO: Compute the update to be applied to the bias term (Eq. 4)\n",
        "    ## We will use this function to update the biases in `train_linear_regression` function (see below):\n",
        "    # new_bias = bias - learning_rate*bias_update(weight, bias, inputs, targets)\n",
        "    ## 0.5 pts\n",
        "    ## Your Code\n",
        "    res = ...\n",
        "    ##########################################\n",
        "    return res\n",
        "\n",
        "def train_linear_regression(inputs: np.array, targets: np.array, weight: float, bias: float, learning_rate: float, update_steps: int = 1000) -> Tuple[float, float, List[float]]:\n",
        "    ## No need to change this function but you need to understand it\n",
        "    losses = []\n",
        "    for _ in range(update_steps):\n",
        "        new_weight = weight - learning_rate*weight_update(weight, bias, inputs, targets)\n",
        "        new_bias = bias - learning_rate*bias_update(weight, bias, inputs, targets)\n",
        "        weight = new_weight\n",
        "        bias = new_bias\n",
        "        losses.append(loss(weight, bias, inputs, targets))\n",
        "    return weight, bias, losses\n"
      ],
      "metadata": {
        "id": "Yf6CSm9J5IXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run gradient descent & Analyze the result (0.5 points)\n",
        "Print out the values of $w$ and $b$ found by your model after training and compare them to the ground truth values (which can be found inside the code of the generate_data function). Are they close? Do you achieve $< 5.2$ MSE loss with your final parameters? If not, optimize your `learning_rate` and `update_steps` hyper-parameters.\n"
      ],
      "metadata": {
        "id": "BszOGvgwyEjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##########################################\n",
        "## TODO: make sure to play around with different learning_rate and update_steps combinations\n",
        "## so that your model achieves <5.2 MSE loss\n",
        "## Make sure to update the parameters\n",
        "learning_rate = 0.001\n",
        "update_steps = 100\n",
        "##########################################\n",
        "\n",
        "new_weight, new_bias, losses = train_linear_regression(inputs, targets, weight, bias, learning_rate, update_steps)\n",
        "print(f\"New weight after training: {new_weight}, new bias after training: {new_bias}\")\n",
        "print(f\"Final loss: {losses[-1]}\")\n",
        "plt.plot(losses)\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Loss\")"
      ],
      "metadata": {
        "id": "HI2RB8P3ZGnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  (Not graded) Recreate the scatter plot you generated in question 1 and plot the model as a line on the same plot. What went wrong?\n"
      ],
      "metadata": {
        "id": "S6_aGorryaH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_model_prediction(x, y):\n",
        "  fig, ax = plt.subplots(figsize=(4, 2))\n",
        "  ##########################################\n",
        "  ## TODO: Create a scatter plot of the data\n",
        "  # (0 points)\n",
        "  ## Your Code\n",
        "\n",
        "  ## TODO: Create a scatter plot of the predicted values\n",
        "  # (0 points)\n",
        "  ## Your Code\n",
        "\n",
        "  ##########################################\n",
        "\n",
        "  ax.set_title(f\"y = {new_weight:.2f} x + {new_bias:.2f}\")\n",
        "\n",
        "  return fig, ax\n",
        "\n",
        "plot_model_prediction(inputs,targets)"
      ],
      "metadata": {
        "id": "ZoysgC4QhGla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 *Robust* Linear Regression (1 Point)\n",
        "Implement a linear regression model exactly like the one you created in the previous question, except using L1 loss (absolute difference) instead of the squared L2 loss (MSE). You should be able to copy and paste your code from question 1.2 and only change a few lines. Print out or plot the loss over the course of training. What is different about the loss trajectory compared to the squared-error linear regression?"
      ],
      "metadata": {
        "id": "vTaZEHQNyejU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def robust_lr_model(weight: float, bias: float, inputs: np.array) -> float:\n",
        "    ##########################################\n",
        "    ## TODO: Implement linear model: y = weight * x + bias (Eq. 1)\n",
        "    ## Hint: linear regression model\n",
        "    ## Your Code, 0.05 points\n",
        "    res = ...\n",
        "    ##########################################\n",
        "    return res\n",
        "\n",
        "def robust_lr_loss(weight: float, bias: float, inputs: np.array, targets: np.array) -> float:\n",
        "    ##########################################\n",
        "    ## TODO: Implement L1 loss\n",
        "    ## Your Code, 0.2 points\n",
        "    res = ...\n",
        "    ##########################################\n",
        "    return res\n",
        "\n",
        "def robust_lr_dloss_dmodel(weight: float, bias: float, inputs: np.array, targets: np.array) -> np.array:\n",
        "    ##########################################\n",
        "    ## TODO: Implement the derivative of the L1 loss function\n",
        "    ## with respect to the model output\n",
        "    ## Your Code, 0.2 points\n",
        "    res = ...\n",
        "    ##########################################\n",
        "    return res\n",
        "\n",
        "def robust_lr_weight_update(weight: float, bias: float, inputs: np.array, targets: np.array) -> float:\n",
        "    ##########################################\n",
        "    ## TODO: Implement the update to be applied to the weights (Eq. 3)\n",
        "    ## We will use this function to update the weights in `train_robust_linear_regression` function (see below):\n",
        "    # new_weight = weight - learning_rate*robust_lr_weight_update(weight, bias, inputs, targets)\n",
        "    ## Your Code, 0.15 points\n",
        "    res = ...\n",
        "    ##########################################\n",
        "    return res\n",
        "\n",
        "def robust_lr_bias_update(weight: float, bias: float, inputs: np.array, targets: np.array) -> float:\n",
        "    ##########################################\n",
        "    ## TODO: Implement the update to be applied to the bias term (Eq. 4)\n",
        "    ## We will use this function to update the biases in `train_robust_linear_regression` function (see below):\n",
        "    # new_bias = bias - learning_rate*robust_lr_bias_update(weight, bias, inputs, targets)\n",
        "    ## Your Code, 0.1 points\n",
        "    res = ...\n",
        "    ##########################################\n",
        "    return res\n",
        "\n",
        "def train_robust_linear_regression(inputs: np.array, targets: np.array, weight: float, bias: float, learning_rate: float, update_steps: int = 1000) -> Tuple[float, float, List[float]]:\n",
        "    ## No need to change this function but please take time to understand it\n",
        "    losses = []\n",
        "    for _ in range(update_steps):\n",
        "        new_weight = weight - learning_rate*robust_lr_weight_update(weight, bias, inputs, targets)\n",
        "        new_bias = bias - learning_rate*robust_lr_bias_update(weight, bias, inputs, targets)\n",
        "        weight = new_weight\n",
        "        bias = new_bias\n",
        "        losses.append(robust_lr_loss(weight, bias, inputs, targets))\n",
        "    return weight, bias, losses\n"
      ],
      "metadata": {
        "id": "iDLr67i4y-fi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run gradient descent & Analyze the result (0.3 points)\n",
        "Print out the new values of $w$ and $b$ found by your model after trainin. Are they closer to the true values used in `generate_data`? Do you achieve $< 1.5$ L1 loss with your final parameters? If not, optimize your `learning_rate` and `update_steps` hyper-parameters.\n",
        "\n",
        "(Not graded) Plot the model as a line again. Why do you think the behavior is different?\n"
      ],
      "metadata": {
        "id": "G4Mr6jKD2rTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##########################################\n",
        "## TODO: make sure to play around with different learning_rate and update_steps combinations\n",
        "## so that your model achieves <1.5 L1 loss\n",
        "## Make sure to update the parameters\n",
        "robust_lr_learning_rate = 0.001\n",
        "robust_lr_update_steps = 100\n",
        "##########################################\n",
        "\n",
        "robust_lr_new_weight, robust_lr_new_bias, robust_lr_losses = train_robust_linear_regression(inputs, targets, weight, bias, robust_lr_learning_rate, robust_lr_update_steps)\n",
        "print(f\"New weight after training: {robust_lr_new_weight}, new bias after training: {robust_lr_new_bias}\")\n",
        "print(f\"Final loss: {robust_lr_loss(robust_lr_new_weight, robust_lr_new_bias, inputs, targets)}\")\n",
        "plt.plot(robust_lr_losses)\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Loss\")"
      ],
      "metadata": {
        "id": "FENG89BU2Pfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_robust_lr_model_prediction(x, y):\n",
        "  fig, ax = plt.subplots(figsize=(4, 2))\n",
        "  ##########################################\n",
        "  ## TODO: Create a scatter plot of the data\n",
        "  # (0 points)\n",
        "  ## Your Code\n",
        "\n",
        "  ## TODO: Create a scatter plot of the predicted values (by the robust_lr model)\n",
        "  # (0 points)\n",
        "  ## Your Code\n",
        "\n",
        "  ##########################################\n",
        "\n",
        "  ax.set_title(f\"y = {new_weight:.2f} x + {new_bias:.2f}\")\n",
        "\n",
        "  return fig, ax\n",
        "\n",
        "plot_robust_lr_model_prediction(inputs,targets)"
      ],
      "metadata": {
        "id": "OlAZqcmtdDG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Implement Logistic Regression (2 Points)\n",
        "In the second part of the homework, you will implement logistic regression."
      ],
      "metadata": {
        "id": "ceJI8TYczR4G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Data Exploration\n",
        "\n",
        "In this section observe the data given to you. You don't need to implement anything here"
      ],
      "metadata": {
        "id": "3_T2iEvvJV7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "def generate_flower_data(n_samples: int = 1000, noise: float = 0.1,\n",
        "                         num_classes: int = 3, seed: int = 42):\n",
        "    rand = np.random.default_rng(seed)\n",
        "    t = rand.uniform(0, 2 * np.pi, n_samples)\n",
        "\n",
        "    # Petal shape: radius varies with class count\n",
        "    r = 1 + 0.3 * np.sin(num_classes * t)\n",
        "\n",
        "    x = r * np.cos(t) + noise * rand.standard_normal(n_samples)\n",
        "    y = r * np.sin(t) + noise * rand.standard_normal(n_samples)\n",
        "\n",
        "    X = np.stack([x, y], axis=1)\n",
        "\n",
        "    # Assign class based on petal angle region\n",
        "    labels = ((t % (2 * np.pi)) / (2 * np.pi) * num_classes).astype(int)\n",
        "    y = labels.reshape(-1, 1)\n",
        "\n",
        "    return shuffle(X, y, random_state=seed)\n",
        "\n",
        "NUM_CLASSES = 4\n",
        "N_FEATURES = 2\n",
        "np.random.seed(SEED)\n",
        "flower_X, flower_y = generate_flower_data(num_classes=NUM_CLASSES)\n"
      ],
      "metadata": {
        "id": "eloVW1d3G6Nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_classification_data(features, labels):\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.scatter(x=features[:, 0], y=features[:, 1], c=labels, cmap=\"viridis\")\n",
        "  plt.title('Flower Dataset')\n",
        "  return fig, ax\n",
        "\n",
        "visualize_classification_data(flower_X, flower_y)"
      ],
      "metadata": {
        "id": "AL7cqS6vHdFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Implementing Logistic Regression\n",
        "\n",
        "Implement a simple logistic regression model using gradient descent to fit the data from the `generate_flower_data` function.\n",
        "Similar to linear regression, use `numpy` only and derive all gradients manually --don't use autograd tools (PyTorch, TensorFlow, MXNet, JAX, etc.).\n",
        "\n",
        "\n",
        "**Model:**\n",
        "In classification problems the goal is to assign each data point to one of the $k$ possible classes. As discussed in class, in logistic (softmax) regression we model this by predicting a probability distribution over $k$ classes for each sample. Given an input $x\\in \\mathbb R^d$, the model computes class probabilities as:\n",
        "$$ \\mathbf{\\hat y} = \\mathrm{softmax} (Wx + b) \\tag{2.1}$$\n",
        "\n",
        "The softmax function maps the output logits $\\mathbf{o}=Wx+b \\in \\mathbb R^k$ to a valid probability distribution:\n",
        "\n",
        "$$ \\mathrm{softmax}(\\mathbf{o})_y = \\frac{\\exp(o_y)}{\\sum_{j=1}^k \\exp(o_j)} , \\tag{2.2}$$\n",
        "\n",
        "where\n",
        "- $\\mathbf{\\hat y}\\in\\mathbb{R}^k$ is the predicted class probabilities\n",
        "- $W\\in \\mathbb{R}^{d\\times k}$ is the weight matrix\n",
        "- $b\\in\\mathbb{R}^k$ is the bias vector\n",
        "- $k$ is the number of classes\n",
        "\n",
        "Remember the softmax function ensures that the model outputs a probability distribution (values in $[0, 1]$ and sum up to 1).\n",
        "\n",
        "1. **Implement the softmax function.**\n",
        "\n",
        "Given a vector of logits $\\mathbf{o}\\in \\mathbb R^k$, compute the softmax output as in (2.2).\n",
        "\n",
        "\n",
        "**Objective:** Minimize the cross-entropy loss between predictions and *one-hot encoded* targets\n",
        "$$\\mathscr{L}(W,b) = -\\frac 1 {n} \\sum_{i=1}^n \\sum_{j=1}^k y_{ij} \\log (\\hat y_{ij}), \\tag{CE}$$\n",
        "where $y\\in\\mathbb R ^{n\\times k}$ is the one-hot encoded label matrix, and $\\hat y\\in\\mathbb R^{n\\times k}$ are the predicted probabilities.\n",
        "\n",
        "2. **Implement the cross-entropy loss**\n",
        "Write a function that computes the cross-entropy loss (CE) given predicted probabilities and one-hot encoded labels.\n",
        "\n",
        "\n",
        "**Updates:** Remember our goal is to find parameters $(W^\\ast, b^\\ast)$ that minimize the objective function.\n",
        "In gradient descent we iteratively update the parameters in the direction of the negative gradient, where\n",
        "$$w_t \\gets W_{t-1} - \\eta \\nabla_{W_{t-1}} \\mathscr L \\tag{2.3}$$\n",
        "$$b_t \\gets b_{t-1} - \\eta \\nabla_{b_{t-1}} \\mathscr L \\tag{2.4}$$\n",
        "\n",
        "\n",
        "**Derivatives:** You must compute:\n",
        "- The intermediate gradient $\\frac {\\partial \\mathscr{L}}{\\partial \\mathbf{o}} \\tag{2.5}$\n",
        "- Gradient of the loss with respect to the weight matrix $W$: $\\frac {\\partial \\mathscr{L}}{\\partial W} \\tag{2.6}$\n",
        "- Gradient of the loss with respect to the bias $b$: $\\frac {\\partial \\mathscr{L}}{\\partial b} \\tag{2.7}$\n",
        "\n",
        "\n",
        "You are welcome to use any technique you want to decide when to stop training. Make sure you tune your optimization hyper-parameters so that the model converges. Print out or plot the loss over the course of training.\n"
      ],
      "metadata": {
        "id": "E0b7J1RaPA9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement Softmax"
      ],
      "metadata": {
        "id": "4PMmKlZbPDds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(logits: np.array) -> np.array:\n",
        "    ##########################################\n",
        "    ## TODO: Implement softmax function\n",
        "    ## Your Code, 0.3 points\n",
        "    ## Hint: Pass keepdims=True\n",
        "    res = ....\n",
        "    ##########################################\n",
        "    # Hint: Remember softmax outputs a vector with same dimensions as the logits\n",
        "    assert logits.shape == res.shape\n",
        "    return res\n",
        "\n",
        "\n",
        "def logistic_reg_model(weight: np.ndarray, bias:np.ndarray, inputs: np.ndarray) -> np.ndarray:\n",
        "    ##########################################\n",
        "    ## TODO: Implement linear model with softmax: y = softmax(X W^T + b)  (Eq 2.1)\n",
        "    ## Your Code, 0.2 points\n",
        "    logits = ...\n",
        "    probs = ...\n",
        "    ##########################################\n",
        "    assert probs.shape == (inputs.shape[0], weight.shape[0])\n",
        "    return probs\n",
        "\n",
        "def cross_entropy_loss(predicted_probs: np.array, targets: np.array) -> float:\n",
        "    n, c = predicted_probs.shape\n",
        "    ## Hint: You can add a small ofset (TODO)\n",
        "    eps = 1e-7\n",
        "    assert predicted_probs.shape == targets.shape\n",
        "    np.testing.assert_allclose(np.sum(predicted_probs, axis=1), 1, rtol=1e-5)\n",
        "    ##########################################\n",
        "    ## TODO: Implement cross entropy loss, 0.3 points\n",
        "\n",
        "    loss = ....\n",
        "    ##########################################\n",
        "    assert isinstance(loss, float), \"CE should return a scalar value.\"\n",
        "    return loss\n",
        "\n",
        "\n",
        "def dloss_dlogits(predicted_probs: np.ndarray, targets: np.ndarray) -> np.ndarray:\n",
        "    ##########################################\n",
        "    ## TODO: Implement derivative of CE loss w.r.t logits (Eq. 2.5)\n",
        "    ## 0.2 pts\n",
        "    ## Your Code\n",
        "    grad = ...\n",
        "    ##########################################\n",
        "    assert grad.shape == predicted_probs.shape\n",
        "    return grad\n",
        "\n",
        "\n",
        "def logistic_reg_weight_update(predicted_probs: np.ndarray, inputs: np.ndarray, targets: np.ndarray) -> np.ndarray:\n",
        "    ##########################################\n",
        "    ## TODO: Gradient of loss w.r.t W (Eq. 2.6)\n",
        "    ## We will use this function to update the weights in `train_logistic_regression` function (see below):\n",
        "    # dW = logistic_reg_weight_update(predicted_probs, inputs, targets)\n",
        "    # weight = weight - learning_rate * dW\n",
        "    ## Hint: you can reuse `dloss_dlogits`\n",
        "    ## 0.3 pts\n",
        "    ## Your Code\n",
        "    dW = ...\n",
        "    ##########################################\n",
        "    return dW\n",
        "\n",
        "\n",
        "def logistic_reg_bias_update(predicted_probs: np.ndarray, inputs: np.ndarray, targets: np.ndarray) -> np.ndarray:\n",
        "    ##########################################\n",
        "    ## TODO: Gradient of loss w.r.t b (Eq. 2.7)\n",
        "    ## We will use this function to update the biases in `train_logistic_regression` function (see below):\n",
        "    # db = logistic_reg_bias_update(predicted_probs, inputs, targets)\n",
        "    # bias = bias - learning_rate * db\n",
        "    ## Hint: you can reuse `dloss_dlogits`\n",
        "    ## 0.2 pts\n",
        "    ## Your Code\n",
        "    db = ...\n",
        "    ##########################################\n",
        "    return db"
      ],
      "metadata": {
        "id": "O2bSoNz6Mhm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_logistic_regression(inputs: np.array, targets: np.array,\n",
        "                              weight: np.ndarray,\n",
        "                              bias: np.ndarray,\n",
        "                              learning_rate: float,\n",
        "                              update_steps: int = 1000) -> Tuple[np.ndarray, np.ndarray, List[float]]:\n",
        "    ## No need to change this function but please take time to understand it\n",
        "    # Make sure targets are one-hot encoded: # X ∈ ℝ^{n×d}, y ∈ one-hot ℝ^{n×k}\n",
        "    from sklearn.preprocessing import OneHotEncoder\n",
        "    targets = OneHotEncoder().fit_transform(targets).toarray()\n",
        "    losses = []\n",
        "    for _ in range(update_steps):\n",
        "        # Forward pass\n",
        "        predicted_probs = logistic_reg_model(weight, bias, inputs)\n",
        "        # Compute gradients\n",
        "        dW = logistic_reg_weight_update(predicted_probs, inputs, targets)\n",
        "        assert dW.shape == weight.shape\n",
        "        db = logistic_reg_bias_update(predicted_probs, inputs, targets)\n",
        "        assert db.shape ==  bias.shape\n",
        "\n",
        "        # Parameter updates\n",
        "        weight = weight - learning_rate * dW\n",
        "        bias = bias - learning_rate * db\n",
        "\n",
        "        # Track loss\n",
        "        losses.append(cross_entropy_loss(predicted_probs, targets))\n",
        "    return weight, bias, losses\n",
        "\n"
      ],
      "metadata": {
        "id": "57SiKrswUrTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run gradient descent & Analyze the result (0.5 points)\n",
        "Do you achieve $< 0.2$ CE loss with your final parameters? If not, optimize your `learning_rate` and `update_steps` hyper-parameters.\n"
      ],
      "metadata": {
        "id": "zRhqqzRb6gNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##########################################\n",
        "## TODO: make sure to play around with different learning_rate and update_steps combinations\n",
        "## so that your model achieves <0.2 CE loss\n",
        "## Make sure to update the parameters\n",
        "log_learning_rate = 0.001\n",
        "log_update_steps = 100\n",
        "##########################################\n",
        "\n",
        "np.random.seed(SEED)\n",
        "log_weight = np.random.randn(NUM_CLASSES, N_FEATURES) * np.sqrt(1.0 / N_FEATURES)\n",
        "log_bias = np.zeros(NUM_CLASSES)\n",
        "\n",
        "new_log_weight, new_log_bias, losses = train_logistic_regression(flower_X, flower_y, log_weight, log_bias, log_learning_rate, log_update_steps)\n",
        "print(f\"Final loss: {losses[-1]}\")\n",
        "plt.plot(losses)\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Loss\")"
      ],
      "metadata": {
        "id": "qgnT7KWi0Clq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(not graded)\n",
        "\n",
        "Plot the predictions using the given function and comment on which samples model gets wrong."
      ],
      "metadata": {
        "id": "ryErumvk3pzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_predictions(inputs: np.ndarray, targets: np.ndarray, weight: np.ndarray, bias: np.ndarray):\n",
        "  y_pred = np.argmax(logistic_reg_model(weight, bias, inputs), axis=1, keepdims=True)\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  correct_mask = (y_pred == targets).flatten()\n",
        "  ax.scatter(inputs[correct_mask, 0], inputs[correct_mask, 1], c=y_pred[correct_mask], cmap=\"viridis\", edgecolors=\"k\", label=\"Correct\")\n",
        "  ax.scatter(\n",
        "        inputs[~correct_mask, 0], inputs[~correct_mask, 1],\n",
        "        c=y_pred[~correct_mask], cmap=\"viridis\",\n",
        "        edgecolors=\"red\", linewidths=1.5, label=\"Wrong\"\n",
        "    )\n",
        "\n",
        "  ax.set_title(\"Logistic Regression Predictions\")\n",
        "  ax.set_xlabel(\"Feature 1\")\n",
        "  ax.set_ylabel(\"Feature 2\")\n",
        "  ax.legend()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "plot_predictions(flower_X, flower_y, new_log_weight, new_log_bias)"
      ],
      "metadata": {
        "id": "BrW12IHT1_Hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collaboration / External Help\n",
        "Disclose any help you used (LLM usage, blogs, search, Github links, etc) and collaborations with your classmates. If you  completed the homework on your own, you can leave this part empty.\n",
        "\n",
        "> TODO"
      ],
      "metadata": {
        "id": "sSSRdUL9DXz4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hh8eSA83Ej0C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}